{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1. Какая модель лучше?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получаем свой корпус текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем тексты с http://fishkamchatka.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEWSPAPER_URL = 'http://fishkamchatka.ru/articles/exclusive/'\n",
    "START_NUM = 15000\n",
    "END_NUM = 30001\n",
    "FILE_NAME = 'fishkamchatka.txt'\n",
    "\n",
    "def download_page(i):\n",
    "    page_link = NEWSPAPER_URL + str(i) + '/'\n",
    "    print(page_link)\n",
    "    r = requests.get(page_link)\n",
    "    if r.status_code != 200:\n",
    "        print(r.status_code)\n",
    "        return\n",
    "    page = html.fromstring(r.text)\n",
    "    elements = page.xpath(\"//div[@class='detail_text_news']//text()\")\n",
    "    text = \"\"\n",
    "    for element in elements:\n",
    "        element = element.strip()\n",
    "        if element:\n",
    "            text += element.strip() + '\\n'\n",
    "    return text\n",
    "\n",
    "def download_corpus():\n",
    "    with open(FILE_NAME, 'w', encoding='utf-8', newline='') as fout:\n",
    "        for i in range(START_NUM, END_NUM):\n",
    "            text = download_page(i)\n",
    "            fout.write(text.strip() + '\\n')\n",
    "#раскомментировать, чтобы запустить\n",
    "#download_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем импорты, определяем вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim\n",
    "import numpy as np\n",
    "from collections import Counter,defaultdict\n",
    "from string import punctuation\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import math\n",
    "\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word != '' and word not in stops]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем предложения из полученного корпуса на токены, нормализуем, удаляем стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(corpus_filename):\n",
    "    data_norm = []\n",
    "    with open(corpus_filename, 'r', encoding='utf-8') as fin:\n",
    "        for index, line in enumerate(fin):\n",
    "            data_norm.append(normalize(line))\n",
    "            if index % 100 == 0:\n",
    "                print(str(index) + ' analyzed')\n",
    "\n",
    "    return data_norm\n",
    "#Раскомментировать, чтобы выполнить\n",
    "#data_norm = normalize_data(FILE_NAME)\n",
    "#Сохраним полученный список в pickle\n",
    "#with open('data_norm.pkl', 'wb') as f:\n",
    "    #pickle.dump(data_norm, f)\n",
    "#Прочитаем из pickle\n",
    "with open('data_norm.pkl', 'rb') as f:\n",
    "    data_norm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рыбопромышленный предприятие регион продолжать наращивать объём вылов производство продукция минприрода хабаровский край рассказать январь май 2016 год компания добыть 154,1 тысяча тонна водный биоресурсы это 1,9 уровень один пять месяц прошлое год',\n",
       " 'председатель комитет рыбный хозяйство сергей рябченко отметить основной усилие промысловик направить вылов нерестовый сельдь охотский мор также добыча палтус треск терпуг кальмар минтай ближний время предприятие готовиться приступить лососёвый путин',\n",
       " 'переработка рыба также продолжать набирать оборот январь-май достигнуть 117 тысяча тонна готовый продукция значительно вырасти объём выпуск рыбопродукт высокий степень переработка 52,2 процент результат общий стоимость произвести продукция возрасти 14,9 процент составить 6,4 миллиард рубль отметить сергей рябченко',\n",
       " 'информация пресс-служба региональный правительство успешный работа предприятие позволить 1,6 раз увеличить налоговый платёж рыбохозяйственный отрасль бюджет хабаровский край пять месяц этот сумма составить 498,1 миллион рубль',\n",
       " 'правительство российский федерация разработать комплекс мера привлечение инвестор рыбоперерабатывающий промышленность дальний восток государственный дума принять законопроект рыболовство сохранение водный биологический ресурс документ вводить новый механизм развитие отрасль оформление квота вылов рыба привлечение вложение такой образ потенциальный инвестор который запрашивать разрешение добыча водный биоресурсы принимать обязательство создать рыбоперерабатывающий мощность приобретать свой деятельность суд российский производство']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219492"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получим word2vec-модель из нашего корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#раскомментировать, чтобы пересчитать\n",
    "#model_fish = gensim.models.Word2Vec([text.split() for text in data_norm], size=50, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним модель в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#раскомментировать, чтобы пересчитать\n",
    "#model_fish.save('model_fish.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fish = gensim.models.Word2Vec.load('model_fish.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('атлантический', 0.7410653233528137),\n",
       " ('тихоокеанский', 0.7378786206245422),\n",
       " ('нерка', 0.7221104502677917),\n",
       " ('кижучить', 0.7164394855499268),\n",
       " ('сёмга', 0.7122551202774048),\n",
       " ('дикий', 0.701239287853241),\n",
       " ('горбуша', 0.6971409916877747),\n",
       " ('лососёвый', 0.684512734413147),\n",
       " ('кета', 0.6739604473114014),\n",
       " ('чавыча', 0.6726918816566467)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fish.wv.most_similar('лосось')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('резиновый', 0.9333532452583313),\n",
       " ('моторный', 0.8869612216949463),\n",
       " ('катер', 0.8731940388679504),\n",
       " ('мотор', 0.8579673171043396),\n",
       " ('надувной', 0.8328689336776733),\n",
       " ('снасть', 0.8186315894126892),\n",
       " ('весельный', 0.8129083514213562),\n",
       " ('бредень', 0.8106197714805603),\n",
       " ('лодочный', 0.8030439019203186),\n",
       " ('квадроцикл', 0.7973795533180237)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fish.wv.most_similar('лодка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ребёнок', 0.7645065784454346),\n",
       " ('семья', 0.7412943243980408),\n",
       " ('голодать', 0.7399080395698547),\n",
       " ('пожилое', 0.7390650510787964),\n",
       " ('радоваться', 0.7368481159210205),\n",
       " ('малообеспеченный', 0.7338917255401611),\n",
       " ('набраться', 0.7308886051177979),\n",
       " ('кормилец', 0.7292143106460571),\n",
       " ('соотечественник', 0.7248889207839966),\n",
       " ('барыш', 0.7243375182151794)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fish.wv.most_similar('человек')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получим модель из rusvectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# возьмем ту же tayga_upos_skipgram_300_2_2019, что в примере\n",
    "model_rusvectores = gensim.models.KeyedVectors.load_word2vec_format('taiga_w2v_model/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('лосось_VERB', 0.7337756156921387),\n",
       " ('горбуша_NOUN', 0.7176412343978882),\n",
       " ('семг_NOUN', 0.6928519010543823),\n",
       " ('осетр_NOUN', 0.6839677095413208),\n",
       " ('рыба_NOUN', 0.661090075969696),\n",
       " ('корюшка_NOUN', 0.6546792984008789),\n",
       " ('форель_NOUN', 0.6532124280929565),\n",
       " ('хариус_NOUN', 0.6493543982505798),\n",
       " ('тунец_NOUN', 0.6446344256401062),\n",
       " ('кета_NOUN', 0.6374989151954651)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rusvectores.most_similar('лосось_NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('шлюпка_NOUN', 0.8258950114250183),\n",
       " ('катер_NOUN', 0.8242191076278687),\n",
       " ('баркас_NOUN', 0.821143627166748),\n",
       " ('лодка_PROPN', 0.7729498147964478),\n",
       " ('плот_NOUN', 0.7696501612663269),\n",
       " ('лодчонка_NOUN', 0.7536875605583191),\n",
       " ('моторка_NOUN', 0.7455837726593018),\n",
       " ('суденышко_NOUN', 0.7437942028045654),\n",
       " ('катамаран_NOUN', 0.7437747716903687),\n",
       " ('ялик_NOUN', 0.7422992587089539)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rusvectores.most_similar('лодка_NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('че-ловек_NOUN', 0.7968202829360962),\n",
       " ('чело-век_NOUN', 0.7886255979537964),\n",
       " ('лю-дь_NOUN', 0.7594989538192749),\n",
       " ('человек_PROPN', 0.7243179082870483),\n",
       " ('людей_NOUN', 0.7150776386260986),\n",
       " ('люди_NOUN', 0.6920535564422607),\n",
       " ('челове-ка_NOUN', 0.6525561213493347),\n",
       " ('человека_NOUN', 0.6498527526855469),\n",
       " ('.человек_NOUN', 0.5669378042221069),\n",
       " ('человеколо_NOUN', 0.5657871961593628)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rusvectores.most_similar('человек_NOUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прочитаем корпус парафразов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
       "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>право полицейский проникновение жилища решить ...</td>\n",
       "      <td>правило внесудебный проникновение полицейский ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             text_1  \\\n",
       "0     0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1     0  Право полицейских на проникновение в жилище ре...   \n",
       "\n",
       "                                              text_2  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...   \n",
       "1  Правила внесудебного проникновения полицейских...   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский разрешить стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилища решить ...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0  полиция мочь разрешить стрелять хулиган травма...  \n",
       "1  правило внесудебный проникновение полицейский ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7227"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['text_1_norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем предсказывать лейблы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизуем тексты парафразов с помощью моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку модель rusvectores имеет частеречные тэги при словах, чтобы применять ее к тексту, нужно для каждого слова текста получить аналогичный тэг (возьмем mystem, как в задании)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "mapping = {}\n",
    "\n",
    "for line in open('ru-rnc.map.txt'):\n",
    "    ms, ud = line.strip('\\n').split()\n",
    "    mapping[ms] = ud\n",
    "\n",
    "def normalize_mystem(text):\n",
    "    tokens = []\n",
    "    norm_words = m.analyze(text)\n",
    "    for norm_word in norm_words:\n",
    "        if 'analysis' not in norm_word:\n",
    "            continue\n",
    "\n",
    "        if not len(norm_word['analysis']):\n",
    "            lemma = norm_word['text']\n",
    "            pos = 'UNKN'\n",
    "        else:\n",
    "            lemma = norm_word[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            if lemma in stops:\n",
    "                continue\n",
    "            pos = norm_word[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "        pos = mapping[pos]\n",
    "        tokens.append(lemma + '_' + pos)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рыба_NOUN', 'ловить_VERB', 'лодка_NOUN']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_mystem('Рыбу ловят с лодки.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для получения векторов для текста из заданной модели (можно передавать нашу, можно готовую)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim, is_mystem):\n",
    "    # is_mystem указывает на то, как разбить текст на токены\n",
    "    if is_mystem:\n",
    "        tokens = normalize_mystem(text)\n",
    "    else:\n",
    "        tokens = text.split()\n",
    "        # чтобы не доставать одно слово несколько раз,\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(tokens)\n",
    "    total = len(tokens)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v * (words[word] / total)  # просто умножаем вектор на частоту\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция получения векторов заданного размера по заданной модели для нашего корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vectors_by_model(data, model, dim, is_mystem):\n",
    "\n",
    "    X_text_1 = np.zeros((len(data['text_1_norm']), dim))\n",
    "    X_text_2 = np.zeros((len(data['text_2_norm']), dim))\n",
    "    for i, text in enumerate(data['text_1_norm'].values):\n",
    "        X_text_1[i] = get_embedding(text, model, dim, is_mystem)\n",
    "        if i % 100 == 0:\n",
    "            print('processed text_1:', i)\n",
    "\n",
    "    for i, text in enumerate(data['text_2_norm'].values):\n",
    "        X_text_2[i] = get_embedding(text, model, dim, is_mystem)\n",
    "        if i % 100 == 0:\n",
    "            print('processed text2:', i)\n",
    "\n",
    "    return X_text_1, X_text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим вектора при помощи rusvectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Раскомментировать, чтобы выполнить\n",
    "#X_text_1_rusvectores, X_text_2_rusvectores = get_vectors_by_model(data, model_rusvectores, 300, True)\n",
    "#Сохраним полученные вектора в pickle\n",
    "#with open('X_text_1_rusvectores.pkl', 'wb') as f:\n",
    "    #pickle.dump(X_text_1_rusvectores, f)\n",
    "\n",
    "#Сохраним полученные вектора в pickle\n",
    "#with open('X_text_2_rusvectores.pkl', 'wb') as f:\n",
    "    #pickle.dump(X_text_2_rusvectores, f)\n",
    "\n",
    "#Прочитаем из pickle\n",
    "with open('X_text_1_rusvectores.pkl', 'rb') as f:\n",
    "    X_text_1_rusvectores = pickle.load(f)\n",
    "\n",
    "#Прочитаем из pickle\n",
    "with open('X_text_2_rusvectores.pkl', 'rb') as f:\n",
    "    X_text_2_rusvectores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим вектора при помощи нашей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Раскомментировать, чтобы выполнить\n",
    "#X_text_1_fish, X_text_2_fish = get_vectors_by_model(data, model_fish, 50, False)\n",
    "#Сохраним полученные вектора в pickle\n",
    "#with open('X_text_1_fish.pkl', 'wb') as f:\n",
    "    #pickle.dump(X_text_1_fish, f)\n",
    "\n",
    "#Сохраним полученные вектора в pickle\n",
    "#with open('X_text_2_fish.pkl', 'wb') as f:\n",
    "    #pickle.dump(X_text_2_fish, f)\n",
    "\n",
    "#Прочитаем из pickle\n",
    "with open('X_text_1_fish.pkl', 'rb') as f:\n",
    "    X_text_1_fish = pickle.load(f)\n",
    "\n",
    "#Прочитаем из pickle\n",
    "with open('X_text_2_fish.pkl', 'rb') as f:\n",
    "    X_text_2_fish = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построим классификаторы на обеих моделях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конкатенируем вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_rusvectores = np.concatenate([X_text_1_rusvectores, X_text_2_rusvectores], axis=1)\n",
    "X_text_fish = np.concatenate([X_text_1_fish, X_text_2_fish], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '0', '0', ..., '-1', '-1', '-1'], dtype=object)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи кросс-валидации оценим качество классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rusvectores = RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "scores_rusvectores = cross_val_score(clf_rusvectores, X_text_rusvectores, y, cv=5, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_fish = RandomForestClassifier(n_estimators=200, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "scores_fish = cross_val_score(clf_fish, X_text_fish, y, cv=5, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения, полученные на rusvectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.47892191, 0.51278507, 0.52249135, 0.38642659, 0.40304709]),\n",
       " 0.46073440273000604)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_rusvectores, np.average(scores_rusvectores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения, полученные на нашей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.44989634, 0.48306842, 0.49965398, 0.35872576, 0.36357341]),\n",
       " 0.4309835805757295)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_fish, np.average(scores_fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель, обученная на небольшом корпусе специфической тематики (рыбная ловля), хуже справляется с перефразированием, чем модель, обученная на достаточно большом корпусе, собранном из интернета. При построении векторов было видно, что многие слова (в основном, географические названия) отсутствуют в корпусе fishkamchatka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2. Обучение по нескольким методам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразуем данные в вектора методом SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим матрицу \"слова на документы\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "X_text_1_matrix = cv.fit_transform(data['text_1_norm'])\n",
    "X_text_2_matrix = cv.fit_transform(data['text_2_norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разложим матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_1_svd = TruncatedSVD(50)\n",
    "X_text_1_svd.fit(X_text_1_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_2_svd = TruncatedSVD(50)\n",
    "X_text_2_svd.fit(X_text_2_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = {i:w for i,w in enumerate(cv.get_feature_names())}\n",
    "word2id = {w:i for i,w in id2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vec_svd_text1 = X_text_1_svd.components_.T\n",
    "id2vec_svd_text2 = X_text_2_svd.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar(word, id2vec):\n",
    "    similar = [id2word[i] for i in cosine_distances(id2vec[word2id[word]].reshape(1, -1), id2vec).argsort()[0][:10]]\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['год',\n",
       " 'уголовный',\n",
       " 'рана',\n",
       " 'последний',\n",
       " 'pussy',\n",
       " 'иран',\n",
       " 'сеть',\n",
       " 'беспорядок',\n",
       " 'папа',\n",
       " 'глава']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('год', id2vec_svd_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['год',\n",
       " '2014',\n",
       " 'приговорить',\n",
       " 'тюрьма',\n",
       " 'посадить',\n",
       " 'карта',\n",
       " 'колония',\n",
       " 'заключение',\n",
       " '2016',\n",
       " 'впервые']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('год', id2vec_svd_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразуем данные в вектора методом NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_nmf = NMF(50)\n",
    "X_text_1_nmf.fit(X_text_1_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=50, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_2_nmf = NMF(50)\n",
    "X_text_2_nmf.fit(X_text_2_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vec_nmf_text1 = X_text_1_nmf.components_.T\n",
    "id2vec_nmf_text2 = X_text_2_nmf.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['год',\n",
       " 'уголовный',\n",
       " 'официальный',\n",
       " 'поддержка',\n",
       " 'беспорядок',\n",
       " 'рана',\n",
       " 'папа',\n",
       " 'иран',\n",
       " 'последний',\n",
       " 'авария']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"год\", id2vec_nmf_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['год',\n",
       " 'приговорить',\n",
       " 'карта',\n",
       " 'тюрьма',\n",
       " 'колония',\n",
       " '2014',\n",
       " 'посадить',\n",
       " 'вывести',\n",
       " 'заключение',\n",
       " 'виновник']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"год\", id2vec_nmf_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получим fasttext-модель из нашего корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#раскомментировать, чтобы пересчитать\n",
    "#model_fish_fasttext = gensim.models.FastText([text.split() for text in data_norm], size=50, min_n=4, max_n=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#раскомментировать, чтобы пересчитать\n",
    "#model_fish_fasttext.save('model_fish_fasttext.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_fish_fasttext = gensim.models.FastText.load('model_fish_fasttext.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('лось', 0.9739271402359009),\n",
       " ('лососина', 0.9495588541030884),\n",
       " ('авось', 0.9270151257514954),\n",
       " ('лосев', 0.873853325843811),\n",
       " ('сосьва', 0.7208265662193298),\n",
       " ('лос-лагос', 0.701827347278595),\n",
       " ('лососёвый', 0.6500760316848755),\n",
       " ('горбуша', 0.6274489164352417),\n",
       " ('гм-сёмга', 0.6055312156677246),\n",
       " ('философия', 0.6042181253433228)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fish_fasttext.wv.most_similar('лосось')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('мотолодка', 0.9670103192329407),\n",
       " ('моторка', 0.9258214831352234),\n",
       " ('подлодка', 0.8962766528129578),\n",
       " ('сводка', 0.8745130300521851),\n",
       " ('катер', 0.8703102469444275),\n",
       " ('сковородка', 0.8615715503692627),\n",
       " ('сплав', 0.8609603047370911),\n",
       " ('наводка', 0.8487827181816101),\n",
       " ('руль-мотор', 0.8264052867889404),\n",
       " ('мотор', 0.8115783333778381)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fish_fasttext.wv.most_similar('лодка')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразуем данные в вектора методом fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем модель, обученную на нашем корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35_64\\lib\\site-packages\\ipykernel\\__main__.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed text_1: 0\n",
      "processed text_1: 100\n",
      "processed text_1: 200\n",
      "processed text_1: 300\n",
      "processed text_1: 400\n",
      "processed text_1: 500\n",
      "processed text_1: 600\n",
      "processed text_1: 700\n",
      "processed text_1: 800\n",
      "processed text_1: 900\n",
      "processed text_1: 1000\n",
      "processed text_1: 1100\n",
      "processed text_1: 1200\n",
      "processed text_1: 1300\n",
      "processed text_1: 1400\n",
      "processed text_1: 1500\n",
      "processed text_1: 1600\n",
      "processed text_1: 1700\n",
      "processed text_1: 1800\n",
      "processed text_1: 1900\n",
      "processed text_1: 2000\n",
      "processed text_1: 2100\n",
      "processed text_1: 2200\n",
      "processed text_1: 2300\n",
      "processed text_1: 2400\n",
      "processed text_1: 2500\n",
      "processed text_1: 2600\n",
      "processed text_1: 2700\n",
      "processed text_1: 2800\n",
      "processed text_1: 2900\n",
      "processed text_1: 3000\n",
      "processed text_1: 3100\n",
      "processed text_1: 3200\n",
      "processed text_1: 3300\n",
      "processed text_1: 3400\n",
      "processed text_1: 3500\n",
      "processed text_1: 3600\n",
      "processed text_1: 3700\n",
      "processed text_1: 3800\n",
      "processed text_1: 3900\n",
      "processed text_1: 4000\n",
      "processed text_1: 4100\n",
      "processed text_1: 4200\n",
      "processed text_1: 4300\n",
      "processed text_1: 4400\n",
      "processed text_1: 4500\n",
      "processed text_1: 4600\n",
      "processed text_1: 4700\n",
      "processed text_1: 4800\n",
      "processed text_1: 4900\n",
      "processed text_1: 5000\n",
      "processed text_1: 5100\n",
      "processed text_1: 5200\n",
      "processed text_1: 5300\n",
      "processed text_1: 5400\n",
      "processed text_1: 5500\n",
      "processed text_1: 5600\n",
      "processed text_1: 5700\n",
      "processed text_1: 5800\n",
      "processed text_1: 5900\n",
      "processed text_1: 6000\n",
      "processed text_1: 6100\n",
      "processed text_1: 6200\n",
      "processed text_1: 6300\n",
      "processed text_1: 6400\n",
      "processed text_1: 6500\n",
      "processed text_1: 6600\n",
      "processed text_1: 6700\n",
      "processed text_1: 6800\n",
      "processed text_1: 6900\n",
      "processed text_1: 7000\n",
      "processed text_1: 7100\n",
      "processed text_1: 7200\n",
      "processed text2: 0\n",
      "processed text2: 100\n",
      "processed text2: 200\n",
      "processed text2: 300\n",
      "processed text2: 400\n",
      "processed text2: 500\n",
      "processed text2: 600\n",
      "processed text2: 700\n",
      "processed text2: 800\n",
      "processed text2: 900\n",
      "processed text2: 1000\n",
      "processed text2: 1100\n",
      "processed text2: 1200\n",
      "processed text2: 1300\n",
      "processed text2: 1400\n",
      "processed text2: 1500\n",
      "processed text2: 1600\n",
      "processed text2: 1700\n",
      "processed text2: 1800\n",
      "processed text2: 1900\n",
      "processed text2: 2000\n",
      "processed text2: 2100\n",
      "processed text2: 2200\n",
      "processed text2: 2300\n",
      "processed text2: 2400\n",
      "processed text2: 2500\n",
      "processed text2: 2600\n",
      "processed text2: 2700\n",
      "processed text2: 2800\n",
      "processed text2: 2900\n",
      "processed text2: 3000\n",
      "processed text2: 3100\n",
      "processed text2: 3200\n",
      "processed text2: 3300\n",
      "processed text2: 3400\n",
      "processed text2: 3500\n",
      "processed text2: 3600\n",
      "processed text2: 3700\n",
      "processed text2: 3800\n",
      "processed text2: 3900\n",
      "processed text2: 4000\n",
      "processed text2: 4100\n",
      "processed text2: 4200\n",
      "processed text2: 4300\n",
      "processed text2: 4400\n",
      "processed text2: 4500\n",
      "processed text2: 4600\n",
      "processed text2: 4700\n",
      "processed text2: 4800\n",
      "processed text2: 4900\n",
      "processed text2: 5000\n",
      "processed text2: 5100\n",
      "processed text2: 5200\n",
      "processed text2: 5300\n",
      "processed text2: 5400\n",
      "processed text2: 5500\n",
      "processed text2: 5600\n",
      "processed text2: 5700\n",
      "processed text2: 5800\n",
      "processed text2: 5900\n",
      "processed text2: 6000\n",
      "processed text2: 6100\n",
      "processed text2: 6200\n",
      "processed text2: 6300\n",
      "processed text2: 6400\n",
      "processed text2: 6500\n",
      "processed text2: 6600\n",
      "processed text2: 6700\n",
      "processed text2: 6800\n",
      "processed text2: 6900\n",
      "processed text2: 7000\n",
      "processed text2: 7100\n",
      "processed text2: 7200\n"
     ]
    }
   ],
   "source": [
    "model_fish_fasttext = gensim.models.Word2Vec.load('model_fish_fasttext.bin')\n",
    "X_text_1_fish_fasttext, X_text_2_fish_fasttext = get_vectors_by_model(data, model_fish_fasttext, 50, False)\n",
    "#Сохраним полученные вектора в pickle\n",
    "with open('X_text_1_fish_fasttext.pkl', 'wb') as f:\n",
    "    pickle.dump(X_text_1_fish_fasttext, f)\n",
    "\n",
    "#Сохраним полученные вектора в pickle\n",
    "with open('X_text_2_fish_fasttext.pkl', 'wb') as f:\n",
    "    pickle.dump(X_text_2_fish_fasttext, f)\n",
    "\n",
    "#Прочитаем из pickle\n",
    "with open('X_text_1_fish_fasttext.pkl', 'rb') as f:\n",
    "    X_text_1_fish_fasttext = pickle.load(f)\n",
    "\n",
    "#Прочитаем из pickle\n",
    "with open('X_text_2_fish_fasttext.pkl', 'rb') as f:\n",
    "    X_text_2_fish_fasttext = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конкатенируем вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_fish_fasttext = np.concatenate([X_text_1_fish_fasttext, X_text_2_fish_fasttext], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучим классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.4, max_features=1000, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "tfidf.fit(pd.concat([data['text_1_norm'], data['text_2_norm']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 100)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_1_svd_text = X_text_1_svd.fit_transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_svd_text = X_text_2_svd.fit_transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_svd_text = np.concatenate([X_text_1_svd_text, X_text_2_svd_text], axis=1)\n",
    "X_svd_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 100)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_1_nmf_text = X_text_1_nmf.fit_transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_nmf_text = X_text_2_nmf.fit_transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_nmf_text = np.concatenate([X_text_1_nmf_text, X_text_2_nmf_text], axis=1)\n",
    "X_nmf_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скомбинируем все полученные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 1000)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_combined = np.concatenate([X_svd_text, X_nmf_text, X_text_fish, X_text_rusvectores, X_text_fish_fasttext], axis=1)\n",
    "X_text_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_combined = RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "scores_combined = cross_val_score(clf_combined, X_text_combined, y, cv=5, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.46302695, 0.50656531, 0.52041522, 0.40166205, 0.4065097 ]),\n",
       " 0.4596358459827622)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_combined, np.average(scores_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат, полученный на скомбинированных векторах, мало отличается от результата на rusvectores, что говорит, вероятно, о высоком качестве модели, обученной на большом корпусе"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
