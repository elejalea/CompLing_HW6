{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import adagram\n",
    "from lxml import html\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word != '' and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def normalize_tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word != '' and word not in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Протестировать адаграм в определении перефразирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем корпус перефразирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель, обученную на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load(\"out.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ловить', 0, 0.8158308),\n",
       " ('берег', 0, 0.5251646),\n",
       " ('река', 0, 0.46946052),\n",
       " ('школа', 0, 0.44177622),\n",
       " ('иордан', 0, 0.43631625),\n",
       " ('западный', 0, 0.42009193),\n",
       " ('оттуда', 0, 0.40351284),\n",
       " ('макс-2001', 0, 0.384777),\n",
       " ('зверь', 0, 0.37941083),\n",
       " ('федеральный', 0, 0.3656322)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.sense_neighbors('рыба', 0) # первое значение слова мир"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, достающая контекст по слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context(i, words, window):\n",
    "    left_context_lb = max(0, i - window)\n",
    "    left_context_rb = i - 1\n",
    "    left_context = []\n",
    "    if left_context_lb <= left_context_rb:\n",
    "        left_context = words[left_context_lb : left_context_rb + 1]\n",
    "    right_context_lb = i+1\n",
    "    right_context_rb = min(len(words), i+1 + window )\n",
    "    right_context = []\n",
    "    if right_context_lb < right_context_rb:\n",
    "        right_context  = words[right_context_lb : right_context_rb]\n",
    "    return left_context + right_context\n",
    "\n",
    "def get_words_in_context(words, window=3):\n",
    "    contexts = []\n",
    "    for i, word in enumerate(words):\n",
    "        contexts.append([word] + [get_context(i, words, window)])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [1, 2, 3]],\n",
       " [1, [0, 2, 3, 4]],\n",
       " [2, [0, 1, 3, 4, 5]],\n",
       " [3, [0, 1, 2, 4, 5, 6]],\n",
       " [4, [1, 2, 3, 5, 6, 7]],\n",
       " [5, [2, 3, 4, 6, 7, 8]],\n",
       " [6, [3, 4, 5, 7, 8, 9]],\n",
       " [7, [4, 5, 6, 8, 9]],\n",
       " [8, [5, 6, 7, 9]],\n",
       " [9, [6, 7, 8]]]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [0,1,2,3,4,5,6,7,8,9]\n",
    "get_words_in_context(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим вектора из adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_adagram(text, model, window, dim):\n",
    "    text_words = text.split(' ')\n",
    "    word2context = get_words_in_context(text_words, window)\n",
    "    vectors = np.zeros((len(word2context), dim))\n",
    "    for i, (word, context) in enumerate(word2context):\n",
    "        try:\n",
    "            best_vector_num = model.disambiguate(word, context).argmax()\n",
    "            v = vm.sense_vector(word, best_vector_num)\n",
    "            vectors[i] = v\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "\n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35_64\\lib\\site-packages\\adagram\\model.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  z = np.log(z)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.14275158, -0.22403363,  0.22693477, -0.52934948,  0.51523248,\n",
       "        0.03478502, -0.37153084, -0.1403941 , -0.22241557,  0.25916778,\n",
       "        0.32752725, -0.7455971 ,  0.12869103,  0.49492278, -0.04040194,\n",
       "       -0.05277447,  0.19305277, -0.3290598 , -0.35874386,  0.04778454,\n",
       "       -0.18476966, -0.32150583, -0.00242544, -0.01663467, -0.26283421,\n",
       "       -0.40783263, -0.18386782,  0.72621819,  0.19111311,  0.39995851,\n",
       "        0.05093759,  0.49570619,  0.42697386, -0.49313649,  0.20842485,\n",
       "       -0.24829472, -0.32408496,  0.34340588, -0.34490345, -0.18698962,\n",
       "       -0.25114506,  0.44190328, -0.27456576,  0.14071238, -0.48556054,\n",
       "       -0.56815771,  0.0659041 , -0.09099596, -0.28906764,  0.33360192,\n",
       "       -0.17318174,  0.60724228, -0.57436242,  0.78905046, -0.41291089,\n",
       "       -0.28126622, -0.1527587 , -0.48059495, -0.49457   , -0.18323295,\n",
       "        0.45138641,  0.17131703, -0.09176735,  0.24156274, -0.61263892,\n",
       "       -0.17049634,  0.33847125, -0.04307706, -0.76868787, -0.46828363,\n",
       "       -0.15922267, -0.06072153, -0.61006693, -0.28956811,  0.55124634,\n",
       "        0.63076669, -0.48250067, -0.24949777,  0.41617149,  0.24410323,\n",
       "        0.3086936 ,  0.15067878,  0.08610045, -0.34726121,  0.03968881,\n",
       "        0.43843007, -0.38116981,  0.00987132,  0.64343679,  0.00385697,\n",
       "        0.08776001, -0.01459433,  0.09314525, -0.17229165, -0.6499759 ,\n",
       "        0.09580523, -0.55213592,  0.1481387 ,  0.4136122 ,  0.46903431])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding_adagram(normalize('Я ловлю рыбу в воде'), vm, 3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vectors_by_model(data, model, window, dim):\n",
    "\n",
    "    X_text_1 = np.zeros((len(data['text_1_norm']), dim))\n",
    "    X_text_2 = np.zeros((len(data['text_2_norm']), dim))\n",
    "    for i, text in enumerate(data['text_1_norm'].values):\n",
    "        X_text_1[i] = get_embedding_adagram(text, model, window, dim)\n",
    "        if i % 100 == 0:\n",
    "            print('processed text_1:', i)\n",
    "\n",
    "    for i, text in enumerate(data['text_2_norm'].values):\n",
    "        X_text_2[i] = get_embedding_adagram(text, model, window, dim)\n",
    "        if i % 100 == 0:\n",
    "            print('processed text2:', i)\n",
    "\n",
    "    return X_text_1, X_text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим вектора при помощи adagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Раскомментировать, чтобы выполнить\n",
    "#X_text_1_adagram, X_text_2_adagram = get_vectors_by_model(data, vm, 3, 100)\n",
    "#Сохраним полученные вектора в pickle\n",
    "#with open('X_text_1_adagram.pkl', 'wb') as f:\n",
    "    #pickle.dump(X_text_1_adagram, f)\n",
    "\n",
    "#Сохраним полученные вектора в pickle\n",
    "#with open('X_text_2_adagram.pkl', 'wb') as f:\n",
    "    #pickle.dump(X_text_2_adagram, f)\n",
    "\n",
    "#Прочитаем из pickle\n",
    "with open('X_text_1_adagram.pkl', 'rb') as f:\n",
    "    X_text_1_adagram = pickle.load(f)\n",
    "\n",
    "#Прочитаем из pickle\n",
    "with open('X_text_2_adagram.pkl', 'rb') as f:\n",
    "    X_text_2_adagram = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конкатенируем вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_adagram = np.concatenate([X_text_1_adagram, X_text_2_adagram], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним классификацию, используя вектора в качестве параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_adagram = RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "scores_adagram = cross_val_score(clf_adagram, X_text_adagram, y, cv=5, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45335176, 0.51347616, 0.5100346 , 0.3968144 , 0.4065097 ])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_adagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Задание 2. Реализовать алгоритм Леска и проверить его на реальном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [word.strip(punct) for word in text.lower().split() if word and word != '']\n",
    "\n",
    "def get_overlapping_words(sentence1_tokenized, sentence2_tokenized):\n",
    "    words1 = set(sentence1_tokenized)\n",
    "    words2 = set(sentence2_tokenized)\n",
    "    return words1.intersection(words2)\n",
    "\n",
    "def lesk( word, sentence_tokenized):\n",
    "    synsets = wn.synsets(word)\n",
    "    bestsense = None\n",
    "    bestsynset = None\n",
    "    best_overlap_num = 0\n",
    "    for i, synset in enumerate(synsets):\n",
    "        definition = synset.definition()\n",
    "        definition_tokenized = tokenize(definition)\n",
    "        overlapping_words = get_overlapping_words(definition_tokenized, sentence_tokenized)\n",
    "        #если везде пересечений нет, возьмем первое значение\n",
    "        if len(overlapping_words) > best_overlap_num or bestsense is None:\n",
    "            best_overlap_num = len(overlapping_words)\n",
    "            bestsense = i\n",
    "            bestsynset = synset\n",
    "    return bestsense, bestsynset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, Synset('day.n.01'))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk('day', 'Earth rotation'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, Synset('day.n.02'))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk('day', 'some point or period in time'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, Synset('day.n.01'))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk('day', 'I am a writer'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим метод на корпусе WSD (возьмем первые 1000 предложений)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_wsd_part = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for i, sent in enumerate(corpus):\n",
    "    if i >= 1000:\n",
    "        break\n",
    "    corpus_wsd_part.append([s.split('\\t') for s in sent.split('\\n')])\n",
    "len(corpus_wsd_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для проверки корпуса через алгоритм Леска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_multisense(word):\n",
    "    return word[0] != ''\n",
    "\n",
    "\n",
    "def get_sentence_tokenized(sentence):\n",
    "    sentence_tokenized = []\n",
    "    for word in sentence:\n",
    "        sentence_tokenized.append(word[1])\n",
    "    return sentence_tokenized\n",
    "\n",
    "\n",
    "def evaluate_corpus_with_lesk(corpus_wsd):\n",
    "    results = []\n",
    "    for i, sentence in enumerate(corpus_wsd):\n",
    "        results += evaluate_sentence_with_lesk(sentence)\n",
    "    return results\n",
    "\n",
    "def evaluate_sentence_with_lesk(sentence):\n",
    "    results = []\n",
    "    sentence_tokenized = get_sentence_tokenized(sentence)\n",
    "    for word in sentence:\n",
    "        if is_multisense(word):\n",
    "            results.append(evaluate_word_with_lesk(word, sentence_tokenized))\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_word_with_lesk(word, sentence_tokenized):\n",
    "    index, synset_by_lesk = lesk(word[1], sentence_tokenized)\n",
    "    synset_key = word[0]\n",
    "    synset_real = wn.lemma_from_key(synset_key).synset()\n",
    "    if synset_by_lesk == synset_real:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(results):\n",
    "    return sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36300738007380073"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_corpus_with_lesk(corpus_wsd_part)\n",
    "evaluate_accuracy(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
